@startuml storage-architecture
!include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Container.puml

LAYOUT_TOP_DOWN()
LAYOUT_WITH_LEGEND()

title Storage Architecture - Data Flow

together {
    System_Boundary(ns_workflows, "Namespace: dtwin-workflows") {
        Container(step1, "Step 1: Preprocessing", "Container", "Transforms input data")
        Container(step2, "Step 2: Model Training", "Container", "Trains ML model")
        Container(step3, "Step 3: Generation", "Container", "Produces output data")
    }
}

System_Boundary(cluster_storage, "Storage Layer") {
    
    System_Boundary(ns_storage, "Namespace: dtwin-storage") {
        ContainerDb(minio, "Object Storage", "MinIO (HA mode)\n3 nodes", "Bucket: workflow-artifacts")
    }
    
    System_Boundary(ns_data, "Namespace: dtwin-data") {
        ContainerDb(postgres, "PostgreSQL", "Primary + Replica", "Workflow metadata\nExecution history")
    }
}

System_Boundary(persistence, "Persistent Volume Layer") {
    ContainerDb(pv_minio, "PVC: minio-data", "StorageClass: ssd\n500Gi", "Artifact storage")
    ContainerDb(pv_postgres, "PVC: postgres-data", "StorageClass: ssd\n100Gi", "Database files")
}

System_Boundary(backup, "Backup & Recovery") {
    ContainerDb(s3_backup, "Backup Bucket", "AWS S3 / Azure Blob", "Daily snapshots\n30-day retention")
}

' Workflow to storage
Rel(step1, minio, "PUT step1-out.csv", "S3")
Rel(step2, minio, "PUT model.pkl", "S3")
Rel(step3, minio, "PUT generated-data.csv", "S3")

' Storage reads (reversed arrows)
Rel(minio, step2, "GET step1-out.csv", "S3")
Rel(minio, step3, "GET model.pkl", "S3")

' Persistence
Rel_D(minio, pv_minio, "Volume mount")
Rel_D(postgres, pv_postgres, "Volume mount")

' Backups
Rel_R(minio, s3_backup, "Nightly sync", "S3")
Rel_R(postgres, s3_backup, "pg_dump daily", "S3")

@enduml
