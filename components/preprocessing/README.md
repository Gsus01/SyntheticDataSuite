# Preprocesador de Series Temporales

Microservicio de preprocesamiento de datos para series temporales, diseÃ±ado para ejecutarse de forma aislada en workflows de Argo. Permite el procesamiento configurable de archivos CSV con mÃºltiples opciones de preprocesamiento especÃ­ficas para datos temporales.

## ğŸš€ CaracterÃ­sticas

- **Preprocesamiento configurable** mediante archivos YAML
- **DetecciÃ³n automÃ¡tica** de columnas temporales
- **Manejo de valores faltantes** con mÃºltiples estrategias
- **DetecciÃ³n y manejo de outliers** (IQR y Z-score)
- **NormalizaciÃ³n de datos** (MinMax, Z-score, Robust)
- **Remuestreo temporal** para datos irregulares
- **Ventanas deslizantes** para anÃ¡lisis temporal
- **SelecciÃ³n de caracterÃ­sticas** configurable
- **EjecuciÃ³n en Docker** para aislamiento completo
- **Tests exhaustivos** con datos de prueba

## ğŸ“‹ Requisitos

- Python 3.11+
- Docker (opcional, para ejecuciÃ³n aislada)
- Dependencias: pandas, numpy, PyYAML

## ğŸ› ï¸ InstalaciÃ³n

### OpciÃ³n 1: EjecuciÃ³n Local

```bash
# Instalar dependencias
pip install -r requirements.txt

# Generar datos de prueba
python generate_test_data.py
```

### OpciÃ³n 2: Docker

```bash
# Construir imagen
docker build -t timeseries-preprocessor .

# O usar el script de utilidad
./run.sh build
```

## ğŸ“– Uso

### EjecuciÃ³n BÃ¡sica

```bash
python preprocess.py --config config.yaml --input data.csv --output processed_data.csv
```

### Con Docker

```bash
docker run --rm \
  -v $(pwd)/config.yaml:/app/config.yaml:ro \
  -v $(pwd)/data.csv:/app/data/input/data.csv:ro \
  -v $(pwd)/output:/app/data/output \
  timeseries-preprocessor \
  --config /app/config.yaml \
  --input /app/data/input/data.csv \
  --output /app/data/output/processed.csv
```

### Con Docker Compose

```bash
# Procesamiento Ãºnico
docker-compose up timeseries-preprocessor

# Procesamiento en lote
docker-compose --profile batch up batch-processor
```

### Script de Utilidad

```bash
# Ejecutar demo completo
./run.sh demo

# Ejecutar tests
./run.sh test

# Procesar archivo especÃ­fico
./run.sh run --input data.csv --output processed.csv

# Procesar con Docker
./run.sh run --input data.csv --output processed.csv --docker
```

## âš™ï¸ ConfiguraciÃ³n

El procesamiento se configura mediante archivos YAML. Ejemplo de configuraciÃ³n:

```yaml
# ConfiguraciÃ³n de logging
logging:
  level: INFO

# ConfiguraciÃ³n de datos
data:
  datetime_column: null  # Auto-detecciÃ³n si es null

# Pipeline de preprocesamiento
preprocessing:
  # SelecciÃ³n de caracterÃ­sticas
  features:
    include: ["timestamp", "value", "sensor_1"]
    exclude: ["id", "metadata"]
    
  # Manejo de valores faltantes
  missing_values:
    strategy: "fill"  # "drop" o "fill"
    method: "interpolate"  # "forward", "backward", "interpolate", "mean"
    threshold: 0.5  # Para strategy="drop"
    
  # DetecciÃ³n de outliers
  outliers:
    enabled: true
    method: "iqr"  # "iqr" o "zscore"
    threshold: 3   # Para mÃ©todo zscore
    action: "cap"  # "remove" o "cap"
    
  # NormalizaciÃ³n
  normalization:
    enabled: true
    method: "minmax"  # "minmax", "zscore", "robust"
    
  # Remuestreo temporal
  resampling:
    enabled: false
    frequency: "1H"  # "1min", "5min", "1H", "1D"
    method: "mean"   # "mean", "sum", "first", "last"
    
  # Ventanas deslizantes
  sliding_windows:
    enabled: true
    size: 5
    features: []  # VacÃ­o = todas las numÃ©ricas
```

## ğŸ“ Estructura del Proyecto

```
components/generation/
â”œâ”€â”€ preprocess.py              # MÃ³dulo principal del preprocesador
â”œâ”€â”€ config.yaml               # ConfiguraciÃ³n por defecto
â”œâ”€â”€ batch_config.yaml         # ConfiguraciÃ³n para lote
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ Dockerfile               # Imagen Docker
â”œâ”€â”€ docker-compose.yml       # OrquestaciÃ³n de contenedores
â”œâ”€â”€ run.sh                   # Script de utilidad
â”œâ”€â”€ generate_test_data.py    # Generador de datos de prueba
â”œâ”€â”€ test_preprocessor.py     # Tests unitarios e integraciÃ³n
â”œâ”€â”€ test_data/              # Datos de prueba
â”‚   â”œâ”€â”€ sensor_data_with_nulls.csv
â”‚   â”œâ”€â”€ financial_data_with_outliers.csv
â”‚   â”œâ”€â”€ irregular_timestamp_data.csv
â”‚   â””â”€â”€ mixed_data_types.csv
â””â”€â”€ README.md               # Este archivo
```

## ğŸ§ª Tests

El proyecto incluye tests exhaustivos con 4 archivos de datos de prueba, cada uno con problemas especÃ­ficos:

### Datos de Prueba

1. **sensor_data_with_nulls.csv**: Datos de sensores IoT con 15% de valores nulos
2. **financial_data_with_outliers.csv**: Datos financieros con outliers extremos (3%)
3. **irregular_timestamp_data.csv**: Timestamps irregulares con gaps y duplicados
4. **mixed_data_types.csv**: Tipos de datos mixtos y formatos inconsistentes

### EjecuciÃ³n de Tests

```bash
# Tests unitarios e integraciÃ³n
python test_preprocessor.py

# O con el script de utilidad
./run.sh test

# Demo con todos los archivos
./run.sh demo
```

## ğŸ³ Docker

### Dockerfile

El contenedor estÃ¡ optimizado para:
- **TamaÃ±o mÃ­nimo**: Base Python 3.11-slim
- **Seguridad**: Usuario no-root
- **Flexibilidad**: VolÃºmenes montables para datos y configuraciÃ³n
- **Logging**: Output sin buffer para mejor observabilidad

### Docker Compose

Incluye dos servicios:
- `timeseries-preprocessor`: Procesamiento Ãºnico
- `batch-processor`: Procesamiento en lote (profile: batch)

## ğŸ”§ IntegraciÃ³n con Argo Workflows

Ejemplo de uso en Argo Workflows:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: preprocess-timeseries
spec:
  entrypoint: preprocess-data
  templates:
  - name: preprocess-data
    container:
      image: timeseries-preprocessor:latest
      command: ["python", "preprocess.py"]
      args: [
        "--config", "/config/config.yaml",
        "--input", "/data/input/raw_data.csv",
        "--output", "/data/output/processed_data.csv"
      ]
      volumeMounts:
      - name: config
        mountPath: /config
      - name: data
        mountPath: /data
  volumes:
  - name: config
    configMap:
      name: preprocessing-config
  - name: data
    persistentVolumeClaim:
      claimName: data-pvc
```

## ğŸ“Š Funcionalidades de Preprocesamiento

### DetecciÃ³n de Columnas Temporales
- Auto-detecciÃ³n por nombre (date, time, timestamp)
- Auto-detecciÃ³n por contenido
- ConversiÃ³n automÃ¡tica a datetime
- IndexaciÃ³n temporal

### Manejo de Valores Faltantes
- **Drop**: Eliminar filas/columnas con umbrales configurables
- **Fill**: Rellenar con forward/backward fill, interpolaciÃ³n o media

### DetecciÃ³n de Outliers
- **IQR Method**: Rango intercuartÃ­lico (Q1-1.5*IQR, Q3+1.5*IQR)
- **Z-Score**: DesviaciÃ³n estÃ¡ndar configurable
- **Acciones**: Eliminar o limitar valores

### NormalizaciÃ³n
- **MinMax**: Escalar a [0,1]
- **Z-Score**: Media 0, desviaciÃ³n 1
- **Robust**: Mediana y rango intercuartÃ­lico

### Remuestreo Temporal
- Frecuencias configurables (minutos, horas, dÃ­as)
- MÃ©todos de agregaciÃ³n (media, suma, primero, Ãºltimo)
- Manejo automÃ¡tico de Ã­ndices temporales

### Ventanas Deslizantes
- Media mÃ³vil y desviaciÃ³n estÃ¡ndar
- TamaÃ±o de ventana configurable
- Aplicable a columnas especÃ­ficas o todas las numÃ©ricas

## ğŸš¨ Manejo de Errores

El preprocesador incluye manejo robusto de errores:
- ValidaciÃ³n de archivos de entrada
- Logging detallado de operaciones
- Manejo de tipos de datos inconsistentes
- RecuperaciÃ³n de errores de parsing temporal

## ğŸ“ˆ Logging y Monitoreo

- Logging estructurado con timestamps
- Niveles configurables (DEBUG, INFO, WARNING, ERROR)
- MÃ©tricas de procesamiento (filas originales vs finales)
- Output compatible con sistemas de monitoreo

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crear rama de feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

Para reportar problemas o solicitar funcionalidades:
1. Crear un issue en el repositorio
2. Incluir logs de error y configuraciÃ³n utilizada
3. Proporcionar datos de ejemplo si es posible
